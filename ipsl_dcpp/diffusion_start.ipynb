{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torchvision\n",
    "import torchvision.transforms as TF\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class BaseConfig:\n",
    "    DEVICE = 'cpu'\n",
    "    DATASET = \"Flowers\"  #  \"MNIST\", \"Cifar-10\", \"Flowers\"\n",
    " \n",
    "    # For logging inferece images and saving checkpoints.\n",
    "    root_log_dir = os.path.join(\"Logs_Checkpoints\", \"Inference\")\n",
    "    root_checkpoint_dir = os.path.join(\"Logs_Checkpoints\", \"checkpoints\")\n",
    " \n",
    "    # Current log and checkpoint directory.\n",
    "    log_dir = \"version_0\"\n",
    "    checkpoint_dir = \"version_0\"\n",
    "\n",
    " \n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    TIMESTEPS = 1000  # Define number of diffusion timesteps\n",
    "    IMG_SHAPE = (1, 32, 32) if BaseConfig.DATASET == \"MNIST\" else (3, 32, 32)\n",
    "    NUM_EPOCHS = 800\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 2e-4\n",
    "    NUM_WORKERS = 2\n",
    "\n",
    "def inverse_transform(tensors):\n",
    "    \"\"\"Convert tensors from [-1., 1.] to [0., 255.]\"\"\"\n",
    "    return ((tensors.clamp(-1, 1) + 1.0) / 2.0) * 255.0\n",
    "\n",
    "def get(element: torch.Tensor, t: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Get value at index position \"t\" in \"element\" and\n",
    "        reshape it to have the same dimension as a batch of images.\n",
    "    \"\"\"\n",
    "    ele = element.gather(-1, t)\n",
    "    return ele.reshape(-1, 1, 1, 1)\n",
    "    \n",
    "class SimpleDiffusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_diffusion_timesteps=1000,\n",
    "        img_shape=(3, 64, 64),\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        self.num_diffusion_timesteps = num_diffusion_timesteps\n",
    "        self.img_shape = img_shape\n",
    "        self.device = device\n",
    "        self.initialize()\n",
    " \n",
    "    def initialize(self):\n",
    "        # BETAs & ALPHAs required at different places in the Algorithm.\n",
    "        self.beta  = self.get_betas()\n",
    "        self.alpha = 1 - self.beta\n",
    "         \n",
    "        self_sqrt_beta                       = torch.sqrt(self.beta).to(self.device)\n",
    "        self.alpha_cumulative                = torch.cumprod(self.alpha, dim=0).to(self.device)\n",
    "        self.sqrt_alpha_cumulative           = torch.sqrt(self.alpha_cumulative).to(self.device)\n",
    "        self.one_by_sqrt_alpha               = 1. / torch.sqrt(self.alpha).to(self.device)\n",
    "        self.sqrt_one_minus_alpha_cumulative = torch.sqrt(1 - self.alpha_cumulative).to(self.device)\n",
    "          \n",
    "    def get_betas(self):\n",
    "        \"\"\"linear schedule, proposed in original ddpm paper\"\"\"\n",
    "        scale = 1000 / self.num_diffusion_timesteps\n",
    "        beta_start = scale * 1e-4\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(\n",
    "            beta_start,\n",
    "            beta_end,\n",
    "            self.num_diffusion_timesteps,\n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "        \n",
    "def forward_diffusion(sd: SimpleDiffusion, x0: torch.Tensor, timesteps: torch.Tensor):\n",
    "    eps = torch.randn_like(x0)  # Noise\n",
    "    mean    = get(sd.sqrt_alpha_cumulative.to(sd.device), t=timesteps) * x0  # Image scaled\n",
    "    std_dev = get(sd.sqrt_one_minus_alpha_cumulative, t=timesteps) # Noise scaled\n",
    "    sample  = mean + std_dev * eps # scaled inputs * scaled noise\n",
    "\n",
    "    return sample, eps  # return ... , gt noise --> model predicts this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from ipsl_dcpp.model.pangu import PanguWeather\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "import lightning as pl\n",
    "\n",
    "import hydra\n",
    "import os\n",
    "os.environ['SLURM_NTASKS_PER_NODE'] = '1'\n",
    "#torch.set_default_dtype(torch.float32)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "#torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "with initialize(version_base=None, config_path=\"conf\"):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "pl.seed_everything(cfg.experiment.seed)\n",
    "train = hydra.utils.instantiate(\n",
    "    cfg.experiment.train_dataset,\n",
    "    generate_statistics=False,\n",
    "    surface_variables=cfg.experiment.surface_variables,\n",
    "    depth_variables=cfg.experiment.depth_variables,\n",
    "    plev_variables=cfg.experiment.plev_variables,\n",
    "    normalization='climatology',\n",
    "    delta=True,\n",
    "    work_path=cfg.environment.work_path,\n",
    "    scratch_path=cfg.environment.scratch_path,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=1\n",
    ")\n",
    "sd = SimpleDiffusion(num_diffusion_timesteps=TrainingConfig.TIMESTEPS, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleDiffusion' object has no attribute 'forward_diffusion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m specific_timesteps:\n\u001b[1;32m      7\u001b[0m     timestep \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(timestep, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m----> 9\u001b[0m     xts, _ \u001b[38;5;241m=\u001b[39m \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_diffusion\u001b[49m(x0s, timestep)\n\u001b[1;32m     10\u001b[0m     xts    \u001b[38;5;241m=\u001b[39m inverse_transform(xts) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     11\u001b[0m     xts    \u001b[38;5;241m=\u001b[39m make_grid(xts, nrow\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimpleDiffusion' object has no attribute 'forward_diffusion'"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "noisy_images = []\n",
    "specific_timesteps = [0, 10, 50, 100, 150, 200, 250, 300, 400, 600, 800, 999]\n",
    " \n",
    "for timestep in specific_timesteps:\n",
    "    timestep = torch.as_tensor(timestep, dtype=torch.long)\n",
    " \n",
    "    xts, _ = sd.forward_diffusion(x0s, timestep)\n",
    "    xts    = inverse_transform(xts) / 255.0\n",
    "    xts    = make_grid(xts, nrow=1, padding=1)\n",
    "     \n",
    "    noisy_images.append(xts)\n",
    "import matplotlib.pyplot as plt\n",
    "_, ax = plt.subplots(1, len(noisy_images), figsize=(10, 5), facecolor='white')\n",
    "\n",
    "for i, (timestep, noisy_sample) in enumerate(zip(specific_timesteps, noisy_images)):\n",
    "    ax[i].imshow(noisy_sample.squeeze(0).permute(1, 2, 0)[:,:,90])\n",
    "    ax[i].set_title(f\"t={timestep}\", fontsize=8)\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].grid(False)\n",
    "\n",
    "plt.suptitle(\"Forward Diffusion Process\", y=0.9)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import MeanMetric\n",
    "\n",
    "def train_one_epoch(model, sd, loader, optimizer, scaler, loss_fn, epoch=800, \n",
    "                   base_config=BaseConfig(), training_config=TrainingConfig()):\n",
    "    \n",
    "    loss_record = MeanMetric()\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=len(loader), dynamic_ncols=True) as tq:\n",
    "        tq.set_description(f\"Train :: Epoch: {epoch}/{training_config.NUM_EPOCHS}\")\n",
    "         \n",
    "        for x in loader:\n",
    "            x0s = x['state_surface'].squeeze()\n",
    "            tq.update(1)\n",
    "            \n",
    "            ts = torch.randint(low=1, high=training_config.TIMESTEPS, size=(x0s.shape[0],), device='cpu')\n",
    "            xts, gt_noise = forward_diffusion(sd, x0s, ts)\n",
    "            print(gt_noise.shape, 'gt_noise')\n",
    "            with amp.autocast():\n",
    "                pred_noise = model(xts, ts).squeeze()\n",
    "                loss = loss_fn(gt_noise, pred_noise)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # scaler.unscale_(optimizer)\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            loss_value = loss.detach().item()\n",
    "            loss_record.update(loss_value)\n",
    "\n",
    "            tq.set_postfix_str(s=f\"Loss: {loss_value:.4f}\")\n",
    "\n",
    "        mean_loss = loss_record.compute().item()\n",
    "    \n",
    "        tq.set_postfix_str(s=f\"Epoch Loss: {mean_loss:.4f}\")\n",
    "    \n",
    "    return mean_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2: Sampling\n",
    "    \n",
    "@torch.inference_mode()\n",
    "def reverse_diffusion(model, sd, timesteps=1000, img_shape=(3, 64, 64), \n",
    "                      num_images=5, nrow=8, device=\"cpu\", **kwargs):\n",
    "\n",
    "    x = torch.randn((num_images, *img_shape), device=device)\n",
    "    model.eval()\n",
    "\n",
    "    if kwargs.get(\"generate_video\", False):\n",
    "        outs = []\n",
    "\n",
    "    for time_step in tqdm(iterable=reversed(range(1, timesteps)), \n",
    "                          total=timesteps-1, dynamic_ncols=False, \n",
    "                          desc=\"Sampling :: \", position=0):\n",
    "\n",
    "        ts = torch.ones(num_images, dtype=torch.long, device=device) * time_step\n",
    "        z = torch.randn_like(x) if time_step > 1 else torch.zeros_like(x)\n",
    "\n",
    "        predicted_noise = model(x, ts)\n",
    "\n",
    "        beta_t                            = get(sd.beta, ts)\n",
    "        one_by_sqrt_alpha_t               = get(sd.one_by_sqrt_alpha, ts)\n",
    "        sqrt_one_minus_alpha_cumulative_t = get(sd.sqrt_one_minus_alpha_cumulative, ts) \n",
    "\n",
    "        x = (\n",
    "            one_by_sqrt_alpha_t\n",
    "            * (x - (beta_t / sqrt_one_minus_alpha_cumulative_t) * predicted_noise)\n",
    "            + torch.sqrt(beta_t) * z\n",
    "        )\n",
    "\n",
    "        if kwargs.get(\"generate_video\", False):\n",
    "            x_inv = inverse_transform(x).type(torch.uint8)\n",
    "            grid = torchvision.utils.make_grid(x_inv, nrow=nrow, pad_value=255.0).to(\"cpu\")\n",
    "            ndarr = torch.permute(grid, (1, 2, 0)).numpy()[:, :, ::-1]\n",
    "            outs.append(ndarr)\n",
    "\n",
    "    if kwargs.get(\"generate_video\", False): # Generate and save video of the entire reverse process. \n",
    "        frames2vid(outs, kwargs['save_path'])\n",
    "        display(Image.fromarray(outs[-1][:, :, ::-1])) # Display the image at the final timestep of the reverse process.\n",
    "        return None\n",
    "\n",
    "    else: # Display and save the image at the final timestep of the reverse process. \n",
    "        x = inverse_transform(x).type(torch.uint8)\n",
    "        grid = torchvision.utils.make_grid(x, nrow=nrow, pad_value=255.0).to(\"cpu\")\n",
    "        pil_image = TF.functional.to_pil_image(grid)\n",
    "        pil_image.save(kwargs['save_path'], format=save_path[-3:].upper())\n",
    "        display(pil_image)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gclyne/miniforge3/envs/env_dcpp/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    BASE_CH = 64  # 64, 128, 256, 512\n",
    "    BASE_CH_MULT = (1, 2, 4, 8) # 32, 16, 8, 4 \n",
    "    APPLY_ATTENTION = (False, False, True, False)\n",
    "    DROPOUT_RATE = 0.1\n",
    "    TIME_EMB_MULT = 2 # 128\n",
    "import torch.nn as nn\n",
    "from torch.cuda import amp\n",
    "from model.unet import UNet2\n",
    "model = UNet2(\n",
    "    n_channels=91,n_out_channels=1,\n",
    ")\n",
    "model.to('cpu')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=TrainingConfig.LR)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 1/800:   1%|          | 1/119 [00:05<09:57,  5.06s/it]/Users/gclyne/miniforge3/envs/env_dcpp/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([91, 143, 144]) gt_noise\n",
      "torch.Size([91, 64, 143, 144])\n",
      "torch.Size([91, 64, 143, 144])\n",
      "torch.Size([91, 1, 143, 144]) pred_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gclyne/miniforge3/envs/env_dcpp/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([91, 1, 143, 144])) that is different to the input size (torch.Size([91, 143, 144])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(1, 2):\n",
    " #   torch.cuda.empty_cache()\n",
    "  #  gc.collect()\n",
    "    \n",
    "    # Algorithm 1: Training\n",
    "    train_one_epoch(model, sd, train_dataloader, optimizer, scaler, loss_fn, epoch=epoch)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      #  save_path = os.path.join(log_dir, f\"{epoch}{ext}\")\n",
    "        \n",
    "        # Algorithm 2: Sampling\n",
    "        reverse_diffusion(model, sd, timesteps=TrainingConfig.TIMESTEPS, num_images=32, generate_video=generate_video,\n",
    "            save_path=save_path, img_shape=TrainingConfig.IMG_SHAPE, device=BaseConfig.DEVICE,\n",
    "        )\n",
    "\n",
    "        # clear_output()\n",
    "        checkpoint_dict = {\n",
    "            \"opt\": optimizer.state_dict(),\n",
    "            \"scaler\": scaler.state_dict(),\n",
    "            \"model\": model.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint_dict, os.path.join(checkpoint_dir, \"ckpt.tar\"))\n",
    "        del checkpoint_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dcpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
